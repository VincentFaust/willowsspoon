# Configuration for airbyte/destination-snowflake
# Documentation about this connector can be found at https://docs.airbyte.com/integrations/destinations/snowflake
resource_name: "snowflake"
definition_type: destination
definition_id: 424892c4-daac-4491-b35d-c6688ba547ba
definition_image: airbyte/destination-snowflake
definition_version: 1.0.2

# EDIT THE CONFIGURATION BELOW!
configuration:
  host: https://ow82791.us-east-2.aws.snowflakecomputing.com # REQUIRED | string | Enter your Snowflake account's <a href="https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#using-an-account-locator-as-an-identifier">locator</a> (in the format <account_locator>.<region>.<cloud>.snowflakecomputing.com) | Examples: accountname.us-east-2.aws.snowflakecomputing.com, accountname.snowflakecomputing.com
  role: sysadmin # REQUIRED | string | Enter the <a href="https://docs.snowflake.com/en/user-guide/security-access-control-overview.html#roles">role</a> that you want to use to access Snowflake | Example: AIRBYTE_ROLE
  schema: raw_source # REQUIRED | string | Enter the name of the default <a href="https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl">schema</a> | Example: AIRBYTE_SCHEMA
  database: spoon_dev # REQUIRED | string | Enter the name of the <a href="https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl">database</a> you want to sync data into | Example: AIRBYTE_DATABASE
  username: vincentffaust # REQUIRED | string | Enter the name of the user you want to use to access the database | Example: AIRBYTE_USER
  warehouse: etl # REQUIRED | string | Enter the name of the <a href="https://docs.snowflake.com/en/user-guide/warehouses-overview.html#overview-of-warehouses">warehouse</a> that you want to sync data into | Example: AIRBYTE_WAREHOUSE
  credentials:
    password: ${PASSWORD} # SECRET (please store in environment variables) | REQUIRED | string | Enter the password associated with the username.
    auth_type: "Username and Password" # OPTIONAL | string
  loading_method:
    ## -------- Pick one valid structure among the examples below: --------
    method: "Standard" # REQUIRED | string
    ## -------- Another valid structure for loading_method: --------
    # method: "Internal Staging" # REQUIRED | string
    ## -------- Another valid structure for loading_method: --------
    # method: "S3 Staging" # REQUIRED | string}
    encryption:
      ## -------- Pick one valid structure among the examples below: --------
      encryption_type: "none" # REQUIRED | string
      ## -------- Another valid structure for encryption: --------
      # encryption_type: "aes_cbc_envelope" # REQUIRED | string
      # key_encrypting_key: ${KEY_ENCRYPTING_KEY} # SECRET (please store in environment variables) | OPTIONAL | string | The key, base64-encoded. Must be either 128, 192, or 256 bits. Leave blank to have Airbyte generate an ephemeral key for each sync.
    # access_key_id: ${ACCESS_KEY_ID} # SECRET (please store in environment variables) | REQUIRED | string | Enter your <a href="https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html">AWS access key ID</a>. Airbyte requires Read and Write permissions on your S3 bucket 
    # s3_bucket_name: # REQUIRED | string | Enter your S3 bucket name | Example: airbyte.staging
    # s3_bucket_region: # OPTIONAL | string | Enter the region where your S3 bucket resides
    # file_name_pattern: # OPTIONAL | string | The pattern allows you to set the file-name format for the S3 staging file(s) | Examples: {date}, {date:yyyy_MM}, {timestamp}, {part_number}, {sync_id}
    # secret_access_key: ${SECRET_ACCESS_KEY} # SECRET (please store in environment variables) | REQUIRED | string | Enter your <a href="https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html">AWS secret access key</a>
    # purge_staging_data: true # OPTIONAL | boolean | Toggle to delete staging files from the S3 bucket after a successful sync
    ## -------- Another valid structure for loading_method: --------
    # method: "GCS Staging" # REQUIRED | string
    # project_id: # REQUIRED | string | Enter the <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">Google Cloud project ID</a> | Example: my-project
    # bucket_name: # REQUIRED | string | Enter the <a href="https://cloud.google.com/storage/docs/creating-buckets">Cloud Storage bucket name</a> | Example: airbyte-staging
    # credentials_json: ${CREDENTIALS_JSON} # SECRET (please store in environment variables) | REQUIRED | string | Enter your <a href="https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys">Google Cloud service account key</a> in the JSON format with read/write access to your Cloud Storage staging bucket
  jdbc_url_params: # OPTIONAL | string | Enter the additional properties to pass to the JDBC URL string when connecting to the database (formatted as key=value pairs separated by the symbol &). Example: key1=value1&key2=value2&key3=value3
  file_buffer_count: 10 # OPTIONAL | integer | Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects | Example: 10
